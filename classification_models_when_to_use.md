# When to use which Classification Technique
---
This table contains the assumptions made by each models about the data and conditions which make it favorable/unfavorable to employ their use them.

| **Model**                  | **Assumptions**                                                                                     | **Favorable Conditions to Use**                                                                                           | **Unfavorable Conditions to Avoid**                                                                                       |
|----------------------------|-----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| **Logistic Regression**    | - Assumes a linear relationship between input features and the log-odds of the outcome.<br>- Features are independent of each other.<br>- No or minimal multicollinearity among predictors.<br>- Normally distributed errors.<br>- Homoscedasticity of errors. | <li> Data is linearly separable or close to linear. <li> Small to medium-sized datasets. <li> Requires probabilistic outputs. <li> When model interpretability is crucial. <li> For binary or multi-class classification tasks. | <li> Highly non-linear relationships between features and output. <li> Complex datasets with high-dimensional data. <li> Multicollinearity among features. <li> High presence of outliers. <li> Imbalanced class distributions without appropriate handling. |
| **Decision Trees**         | - Assumes the data can be split hierarchically into homogeneous subgroups.<br>- Features are conditionally independent within branches.<br>- Data has sufficient diversity for splits.<br>- Does not assume a linear relationship.<br>- Splitting criteria (e.g., Gini index, entropy) drive tree construction. | <li> Non-linear relationships between features and outcomes. <li> Mixed data types (categorical and numerical). <li> Requires easy interpretation and visualization. <li> Small to medium-sized datasets. <li> When feature engineering is difficult or unavailable. | <li> Very large datasets without pruning or regularization. <li> High-dimensional data leading to overfitting. <li> Sensitive to small changes in data (instability). <li> Biased towards features with more levels. <li> Not ideal for datasets requiring high predictive accuracy without ensemble methods. |
| **Random Forest**          | - Assumes multiple decision trees combined will improve model stability and accuracy.<br>- Assumes each tree can learn different aspects of the data.<br>- Features are randomly selected for splits in each tree.<br>- Combines outputs via averaging (regression) or majority voting (classification).<br>- Reduces variance compared to single decision trees. | <li> Large datasets with many features. <li> High-dimensional data where feature selection is critical. <li> Need for robust, non-linear classification. <li> When overfitting is a concern with single models. <li> Mixed data types (categorical and numerical). | <li> High computational and memory cost for large datasets. <li> When interpretability of individual features is critical. <li> Sparse datasets with few data points relative to features. <li> Highly imbalanced class distributions. <li> Presence of noise in data, which can reduce ensemble performance. |
| **Support Vector Machine (SVM)** | - Assumes data can be separated by a hyperplane (linear or kernel-transformed).<br>- Works best with a clear margin of separation.<br>- Kernel trick assumes feature spaces can be transformed for better separability.<br>- Data must be normalized.<br>- Assumes balanced datasets for better performance. | <li> High-dimensional feature spaces. <li> When the dataset is relatively small. <li> Non-linear relationships (with kernel trick). <li> Requires robust performance on well-separated data. <li> Scenarios needing effective regularization. | <li> Large datasets due to high computational complexity. <li> Overlapping classes with no clear margin of separation. <li> Noise in data leading to reduced accuracy. <li> Imbalanced datasets without appropriate handling. <li> Difficulty in tuning hyperparameters (e.g., kernel, C). |
| **Feedforward Neural Networks (FNNs)** | - Assumes input-output relationships can be approximated using non-linear transformations.<br>- Relies on sufficient labeled data for learning.<br>- Requires preprocessed and normalized input data.<br>- Network architecture determines approximation ability.<br>- Errors are minimized through iterative gradient-based optimization. | <li> Complex, non-linear classification tasks. <li> Large datasets with sufficient labeled data. <li> Problems requiring custom architectures and loss functions. <li> When capturing interactions between features is crucial. <li> Scenarios requiring multi-class classification. | <li> Small datasets due to overfitting risk. <li> High computational requirements without access to GPUs. <li> Problems with unbalanced datasets. <li> Noisy datasets or irrelevant features without feature selection. <li> Tasks requiring immediate model interpretability. |
| **Convolutional Neural Networks (CNNs)** | - Assumes input data has spatial structure (e.g., images).<br>- Learns spatial hierarchies through convolutional layers.<br>- Assumes local spatial correlation.<br>- Requires sufficient labeled training data.<br>- Kernel filters assume translational invariance in features. | <li> Image or spatial data classification tasks. <li> Large datasets with high-resolution images. <li> Requires automatic feature extraction. <li> Tasks involving local feature detection (e.g., edges, textures). <li> Need for high accuracy in computer vision applications. | <li> Non-image data or tabular datasets. <li> Limited labeled data or small datasets. <li> High computational resource requirements (e.g., GPUs). <li> Tasks requiring quick prototyping and simple models. <li> Problems where model explainability is critical. |
| **Recurrent Neural Networks (RNNs)** | - Assumes input data is sequential or time-dependent.<br>- Captures dependencies across sequential steps.<br>- Memory cells assume importance of past data.<br>- Relies on labeled sequential data for training.<br>- Errors are propagated through time for weight updates. | <li> Time-series or sequence-based data. <li> Problems requiring modeling of temporal dependencies. <li> Scenarios with long sequential data (e.g., LSTMs/GRUs). <li> Tasks requiring contextual understanding of previous inputs. <li> Applications in speech, text, or sensor data classification. | <li> Non-sequential data like tabular datasets. <li> Long sequences leading to vanishing gradients in vanilla RNNs. <li> High computational requirements for training. <li> Scenarios requiring quick prototyping and simple solutions. <li> Large datasets where computational efficiency is critical. |
| **Transformers (e.g., BERT, GPT)** | - Assumes self-attention mechanism can model dependencies effectively.<br>- Requires sequential or text-based data.<br>- Works best with pre-trained weights on large corpora.<br>- Captures long-range dependencies in sequences.<br>- Relies on massive labeled or unlabeled datasets for pretraining. | <li> NLP tasks like text classification, sentiment analysis, or translation. <li> Problems requiring long-range dependency capture. <li> Pre-trained models can be fine-tuned for specific tasks. <li> High-resource environments with access to GPUs. <li> Need for state-of-the-art performance in NLP. | <li> Non-text or non-sequential data. <li> Small datasets due to risk of overfitting. <li> Limited computational resources. <li> Niche or domain-specific tasks requiring extensive fine-tuning. <li> Problems requiring immediate model interpretability. |

---