# When to use which Regression Model

This table contains the assumptions made by each models about the data and conditions which make it favorable/unfavorable to employ their use them.

---

| **Model**                  | **Assumptions**                                                                                     | **Favorable Conditions to Use**                                                                                           | **Unfavorable Conditions to Avoid**                                                                                       |
|----------------------------|-----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| **Linear Regression**      | - Assumes a linear relationship between features and target.<br>- Features are independent.<br>- Errors are normally distributed.<br>- Errors have constant variance (homoscedasticity).<br>- Minimal multicollinearity among predictors. | <li> Linear relationships in the data. <li> Small to medium-sized datasets. <li> Easy interpretability of model coefficients. <li> When computational efficiency is required. <li> Feature importance is desired. | <li> Highly non-linear data relationships. <li> High presence of outliers. <li> Severe multicollinearity among predictors. <li> Heteroscedasticity in the data. <li> Data with missing values or extreme imbalance. |
| **Lasso Regression (L1)**  | - Assumes sparse feature relationships (some coefficients shrink to zero).<br>- Linear relationship between features and target.<br>- Normal distribution of errors.<br>- Assumes penalization improves generalization. | <li> Feature selection is important. <li> Data has many irrelevant or insignificant features. <li> Small to medium-sized datasets. <li> When overfitting is a concern. <li> Requires a simple, interpretable model. | <li> Highly non-linear data. <li> Features with high collinearity (bias introduced). <li> Data requiring non-sparse solutions. <li> Small datasets with noisy features. <li> Cases requiring no penalization. |
| **Ridge Regression (L2)**  | - Assumes all features contribute to the target but need regularization.<br>- Linear relationships between features and target.<br>- Penalizes large coefficients to reduce overfitting.<br>- Normally distributed errors. | <li> Linear relationships in data with some collinearity. <li> Requires robustness to multicollinearity. <li> Small to medium-sized datasets. <li> Preventing overfitting while using all features. <li> Feature importance is needed. | <li> Highly non-linear data. <li> Data with excessive noise. <li> Very large datasets where computation becomes costly. <li> When sparse solutions are preferred. <li> Strong dependency on feature scaling. |
| **Support Vector Regression (SVR)** | - Assumes data can be separated with a hyperplane using a kernel.<br>- Works well with a clear margin of error (epsilon).<br>- Assumes importance of a subset of data points (support vectors).<br>- Requires normalized input features.<br>- Assumes balanced datasets. | <li> High-dimensional data. <li> Non-linear relationships (with kernel trick). <li> Problems with small to medium-sized datasets. <li> Scenarios requiring robust regression. <li> Data with minimal noise. | <li> Large datasets due to high computational complexity. <li> Highly noisy data. <li> Strong overlapping between feature contributions. <li> High sensitivity to hyperparameter tuning. <li> Imbalanced or unscaled features. |
| **Random Forest Regressor** | - Assumes ensembling of decision trees reduces variance and overfitting.<br>- Assumes the target is a non-linear function of inputs.<br>- Features are randomly selected at each split.<br>- No assumptions about feature independence.<br>- Assumes sufficient data diversity for robust splits. | <li> Non-linear relationships in the data. <li> Large datasets with high-dimensional features. <li> Need for robustness to overfitting. <li> Scenarios where interpretability of feature importance is critical. <li> Problems with mixed data types. | <li> Sparse datasets with low diversity. <li> High computational and memory costs. <li> Highly imbalanced datasets without appropriate adjustments. <li> Sensitivity to noisy data. <li> Problems requiring highly interpretable models. |
| **Gradient Boosting Regressor (GBR)** | - Assumes additive models can approximate the target.<br>- Weak learners are combined sequentially to minimize residuals.<br>- Works well with non-linear relationships.<br>- Assumes overfitting is manageable with regularization.<br>- Requires clean, preprocessed data. | <li> Non-linear relationships in data. <li> Medium to large datasets. <li> Scenarios requiring high predictive accuracy. <li> Datasets with moderate noise. <li> When feature importance and interpretability are needed. | <li> Highly noisy or small datasets (risk of overfitting). <li> High computational cost for large datasets. <li> Requires careful hyperparameter tuning. <li> Imbalanced datasets without adjustments. <li> Sparse datasets with limited diversity. |
| **Feedforward Neural Networks (FNNs)** | - Assumes non-linear relationships between inputs and target.<br>- Requires sufficient labeled data.<br>- Errors are minimized iteratively via gradient descent.<br>- Network size affects model capacity.<br>- Assumes preprocessed and normalized input data. | <li> Complex, non-linear regression tasks. <li> Large datasets with sufficient labeled examples. <li> Scenarios where capturing feature interactions is crucial. <li> Requires multi-output regression. <li> Tasks needing customizable architectures. | <li> Small datasets due to overfitting risks. <li> High computational requirements without GPU access. <li> Data with excessive noise or irrelevant features. <li> Highly interpretable regression tasks. <li> Datasets requiring quick prototyping. |
| **Convolutional Neural Networks (CNNs)** | - Assumes spatial or structured input data.<br>- Uses convolutional layers to detect local patterns.<br>- Learns hierarchical features automatically.<br>- Kernel filters assume translational invariance.<br>- Assumes sufficient labeled data for training. | <li> Spatial data, e.g., images or video regression tasks. <li> Large datasets with structured patterns. <li> Problems requiring hierarchical feature learning. <li> Scenarios requiring high predictive accuracy. <li> Regression tasks with spatial relationships. | <li> Tabular datasets without spatial structure. <li> Small datasets due to overfitting risks. <li> High computational and memory requirements. <li> Problems requiring immediate interpretability. <li> Tasks with minimal spatial correlation. |
| **Transformers (e.g., BERT)** | - Assumes data has sequential or contextual dependencies.<br>- Relies on self-attention for feature extraction.<br>- Assumes sufficient labeled or pre-trained data.<br>- Captures long-range dependencies effectively.<br>- Requires normalized input sequences. | <li> Sequence-based regression tasks (e.g., time-series). <li> Long-range dependency capture is crucial. <li> Problems requiring state-of-the-art accuracy. <li> Large labeled datasets or pre-trained models available. <li> Scenarios involving structured or sequential data. | <li> Non-sequential or tabular data. <li> Small datasets due to risk of overfitting. <li> High computational and memory costs. <li> Scenarios needing quick prototyping or simple solutions. <li> Regression tasks requiring high interpretability. |

---
