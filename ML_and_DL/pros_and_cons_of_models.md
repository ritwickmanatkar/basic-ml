# Pros and Cons of ML, and DL Algorithms

This document outlines the strengths and weaknesses of various traditional machine learning models, advanced machine learning techniques, popular deep learning architectures, and gradient boosting techniques.

---

## Traditional Machine Learning Models

| **Model Type**              | **Strengths**                                                                                                 | **Weaknesses**                                                                                                                                                    |
|------------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Linear Regression**        | <li> Simple to implement and interpret. <li> Computationally efficient and works well on small datasets. <li> Provides feature importance through coefficients. | <li> Assumes a linear relationship, which might not hold for complex datasets. <li> Sensitive to outliers, which can disproportionately influence the model. <li> Struggles with multicollinearity among predictors.   |
| **L1-Regularized Regression (Lasso)** | <li> Performs feature selection by shrinking some coefficients to zero. <li> Prevents overfitting through regularization. <li> Encourages sparse solutions for interpretability. | <li> May discard useful features entirely if they are correlated. <li> Can be unstable with highly correlated features. <li> May lead to underfitting if the regularization parameter is too high. |
| **L2-Regularized Regression (Ridge)** | <li> Handles multicollinearity better by shrinking coefficients uniformly. <li> Reduces overfitting for complex models. <li> More stable than Lasso with correlated features. | <li> Does not perform feature selection (all coefficients are shrunk but not zeroed). <li> Model complexity increases with more features. <li> Requires tuning of the regularization parameter for optimal performance. |
| **Logistic Regression**      | <li> Simple, fast, and interpretable for binary and multi-class classification. <li> Provides probabilities, making it useful for ranking and decision-making tasks. <li> Works well with small datasets and is computationally efficient. | <li> Assumes a linear decision boundary, which may not work for complex datasets. <li> Sensitive to multicollinearity among predictors. <li> Requires manual feature engineering for non-linear relationships. |
| **Decision Trees**           | <li> Intuitive and easy to interpret. <li> Handles both numerical and categorical features natively. <li> Captures non-linear relationships effectively. | <li> Prone to overfitting, especially without pruning or regularization. <li> Sensitive to small changes in the data (results in instability). <li> Tends to be biased towards features with more levels in categorical data. |
| **K-Nearest Neighbors (KNN)**| <li> Simple and easy to implement, with no explicit training phase. <li> Effective for non-linear decision boundaries and multi-class classification. <li> Can handle noisy and overlapping data to some extent. | <li> Computationally expensive during prediction as it requires calculating distances for all training points. <li> Highly sensitive to irrelevant features and feature scaling. <li> Struggles with large datasets due to memory and computation requirements. |

---

## Clustering Techniques

| **Clustering Algorithm**     | **Strengths**                                                                                                 | **Weaknesses**                                                                                                                                                    |
|-------------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **K-Means Clustering**        | <li> Simple, fast, and works well with large datasets. <li> Works well when clusters are spherical and evenly distributed. <li> Scales well to higher-dimensional data. | <li> Requires specifying the number of clusters in advance. <li> Struggles with non-spherical or overlapping clusters. <li> Sensitive to initialization of centroids and outliers. |
| **DBSCAN**                    | <li> Can detect clusters of arbitrary shape. <li> Robust to noise and outliers. <li> Does not require the number of clusters to be specified. | <li> Struggles with varying density clusters. <li> Requires careful tuning of hyperparameters (e.g., Îµ and minPts). <li> May not scale well to large datasets with high dimensions. |
| **Hierarchical Clustering**   | <li> Does not require pre-specifying the number of clusters. <li> Provides a dendrogram for understanding relationships among data points. <li> Flexible with different linkage criteria (e.g., single, complete, average). | <li> Computationally expensive for large datasets. <li> Sensitive to noise and outliers, which can distort the hierarchy. <li> Struggles with high-dimensional data. |

---

## Popular Deep Learning Architectures

| **Architecture**             | **Strengths**                                                                                                 | **Weaknesses**                                                                                                                                                    |
|-------------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Feedforward Neural Networks (FNNs)** | <li> Handles complex, non-linear relationships effectively. <li> Can model multi-output regression/classification. <li> Supports custom architectures and loss functions. | <li> Requires a large amount of data for good performance. <li> Prone to overfitting without regularization techniques like dropout or early stopping. <li> Computationally expensive and slower to train compared to traditional ML models. |
| **Convolutional Neural Networks (CNNs)** | <li> Excellent for image and spatial data. <li> Learns spatial hierarchies automatically. <li> Invariant to translation and scaling in images. | <li> Requires significant computational resources (e.g., GPUs). <li> Struggles with non-spatial data like tabular data. <li> Sensitive to hyperparameters like filter size, stride, and pooling size. |
| **Recurrent Neural Networks (RNNs)** | <li> Handles sequential data (e.g., time series, text) effectively. <li> Captures temporal dependencies in data. <li> Flexible for diverse sequence data. | <li> Suffers from vanishing and exploding gradient problems in long sequences. <li> Struggles with long-term dependencies (mitigated by LSTMs/GRUs). <li> Slower training due to sequential data processing. |
| **Transformers (e.g., BERT, GPT)** | <li> State-of-the-art for NLP tasks (e.g., translation, summarization). <li> Handles long-range dependencies effectively using self-attention. <li> Pre-trained models allow efficient transfer learning. | <li> Extremely resource-intensive to train and deploy. <li> Requires a large amount of labeled data for fine-tuning. <li> May overfit on small datasets or underperform on niche tasks without proper fine-tuning. |

---

## Popular Gradient Boosting Techniques

| **Technique**                | **Strengths**                                                                                                 | **Weaknesses**                                                                                                                                                    |
|-------------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **XGBoost**                  | <li> Highly efficient, fast, and scalable implementation of gradient boosting. <li> Regularization terms (L1 and L2) reduce overfitting and improve generalization. <li> Built-in handling of missing values. | <li> Tuning hyperparameters (e.g., learning rate, max depth) can be complex and time-consuming. <li> Memory-intensive, especially on large datasets. <li> Sensitive to feature scaling and noise in the data. |
| **LightGBM**                 | <li> Handles large datasets efficiently by using histogram-based binning. <li> Faster training compared to XGBoost, especially for high-dimensional datasets. <li> Supports categorical features natively, without needing one-hot encoding. | <li> Can struggle with small datasets as the histogram-based approach may lose precision. <li> Sensitive to overfitting on noisy datasets or small data. <li> May not perform as well as CatBoost when dealing with high-cardinality categorical variables. |
| **CatBoost**                 | <li> Natively handles categorical features without preprocessing (like one-hot encoding or target encoding). <li> Robust to overfitting, even with small datasets, due to ordered boosting. <li> Works well with imbalanced datasets out of the box. | <li> Slower training time compared to LightGBM when categorical variables are not prominent. <li> Limited support for distributed training compared to XGBoost and LightGBM. <li> Higher computational cost per iteration compared to histogram-based approaches like LightGBM. |
| **HistGradientBoosting**     | <li> Implementation is optimized for speed with histogram-based binning similar to LightGBM. <li> Easy integration with other Scikit-learn tools and pipelines. <li> Competitive accuracy on small and medium-sized datasets. | <li> Lacks some advanced features provided by XGBoost and CatBoost, such as GPU acceleration. <li> Performance on large datasets may be slower compared to specialized libraries. <li> Limited support for distributed training, making it less suitable for very large datasets. |

---